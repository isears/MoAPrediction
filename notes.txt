Original model (baseline 100 epochs, dropout, 1 hidden layer): cv 0.01641549877
    - Disable optimizer: cv 0.04488035663962364
    - Optimizer with default options: cv 0.019868889823555946
    - 5 Hidden layers: cv 0.016846954822540283
    - 10 Hidden layers: cv ~ 0.018
    - 3 Hidden layers: cv ~ 0.01637718975543976
        - w/softmax in final layer: training loss ~ 0.0144, cv ~ 0.016888035461306572
    - 4 'pyramid' layers: cv 0.0164251733571291
    - 5 Hidden layers, no dropout: cv 0.01618672236800194
        - w/softmax in final layer: training loss ~ 0.0149, cv ~0.016556181013584137
        - w/adam optimizer & 0.1 dropout: training loss ~ 0.00449, cv ~0.03500184416770935
            - w/out dropout: training loss ~ 0.00205, cv ~0.04243400692939758
            - 0.3 dropout 30 epochs (early stopping): trainling loss ~ 0.0155, cv ~ 0.01681930013000965
    - 3 Hidden layers, no dropout: cv 0.01619345657527447
    - 10 Hidden layers, no dropout: cv 0.016700008884072304
    - 4 Hidden layers, no dropout, 1000 epochs: training loss ~ 4.92e-5, cv 0.061739494651556016
        - Massive overfitting
    - 5 Hidden layers, no dropout, 500 epochs: cv 0.04931907877326012
    - 100 Hidden layers, no dropout, 500 epochs: cv ~ 0.020841794088482857

Original nonscored model (30 epochs, no dropout, 5 hidden layers, SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)): cv 0.004474121052771807

Original nonscored ensemble (30 epochs, no dropout, 5 hidden layers, SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)): cv 0.0178315207362175
    - With/perfect nonscored predictions: cv 0.017839355394244194
    - 100 epochs: 0.01641344577074051
        - 100 epochs/adm optimizer: massive divergence from training after only 10 epochs
            - 100 epochs/adm optimizer w/dropout (0.3): gets down to ~0.0164 @ 20 epochs and becomes unstable
                - w/lr 1e-4: down to 0.0174 @ 20 epochs, then diverges
            - 100 epochs/adm optimizer w/lr 1e-4: gets down to ~0.0163 @ 7 epochs, then diverges
        - 100 epcohs w/dropout (0.3): cv ~0.0203, train ~0.0192 v. slow
            - lr 0.1: divergence at cv ~0.017
        - shape change to 2000 input layer, 1500 hidden layers: ~ 0.016204802319407463 (1 round, long train time)
            - reduce hidden layers to 3: cv ~0.01608872227370739 (basically the same)
            - increase hidden layers to 10: cv ~0.016980761662125587

Ideas for future:
    - Train single-label classification models for "highest yield" labels
        - Most popular labels in training data
        - Most difficult labels to predict correctly